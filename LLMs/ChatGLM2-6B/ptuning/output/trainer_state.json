{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 30.131826741996232,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.3013182674199623,
      "grad_norm": 0.041579462587833405,
      "learning_rate": 0.0099,
      "loss": 4.4638,
      "step": 10
    },
    {
      "epoch": 0.6026365348399246,
      "grad_norm": 0.042861633002758026,
      "learning_rate": 0.0098,
      "loss": 3.3899,
      "step": 20
    },
    {
      "epoch": 0.903954802259887,
      "grad_norm": 0.019731702283024788,
      "learning_rate": 0.0097,
      "loss": 2.9816,
      "step": 30
    },
    {
      "epoch": 1.2052730696798493,
      "grad_norm": 0.00803113542497158,
      "learning_rate": 0.0096,
      "loss": 2.8857,
      "step": 40
    },
    {
      "epoch": 1.5065913370998116,
      "grad_norm": 0.006006977520883083,
      "learning_rate": 0.0095,
      "loss": 2.8,
      "step": 50
    },
    {
      "epoch": 1.807909604519774,
      "grad_norm": 0.006892246659845114,
      "learning_rate": 0.0094,
      "loss": 2.828,
      "step": 60
    },
    {
      "epoch": 2.109227871939736,
      "grad_norm": 0.0068437703885138035,
      "learning_rate": 0.009300000000000001,
      "loss": 2.7785,
      "step": 70
    },
    {
      "epoch": 2.4105461393596985,
      "grad_norm": 0.006782567128539085,
      "learning_rate": 0.0092,
      "loss": 2.7223,
      "step": 80
    },
    {
      "epoch": 2.711864406779661,
      "grad_norm": 0.011153936386108398,
      "learning_rate": 0.0091,
      "loss": 2.7488,
      "step": 90
    },
    {
      "epoch": 3.0131826741996233,
      "grad_norm": 0.016328509896993637,
      "learning_rate": 0.009000000000000001,
      "loss": 2.766,
      "step": 100
    },
    {
      "epoch": 3.3145009416195856,
      "grad_norm": 0.02455553039908409,
      "learning_rate": 0.0089,
      "loss": 2.789,
      "step": 110
    },
    {
      "epoch": 3.615819209039548,
      "grad_norm": 0.015264970250427723,
      "learning_rate": 0.0088,
      "loss": 2.6697,
      "step": 120
    },
    {
      "epoch": 3.9171374764595104,
      "grad_norm": 0.021795786917209625,
      "learning_rate": 0.0087,
      "loss": 2.7002,
      "step": 130
    },
    {
      "epoch": 4.218455743879472,
      "grad_norm": 0.022938553243875504,
      "learning_rate": 0.0086,
      "loss": 2.7149,
      "step": 140
    },
    {
      "epoch": 4.519774011299435,
      "grad_norm": 0.032675404101610184,
      "learning_rate": 0.0085,
      "loss": 2.7192,
      "step": 150
    },
    {
      "epoch": 4.821092278719397,
      "grad_norm": 0.03407915309071541,
      "learning_rate": 0.0084,
      "loss": 2.6928,
      "step": 160
    },
    {
      "epoch": 5.12241054613936,
      "grad_norm": 0.03350664675235748,
      "learning_rate": 0.0083,
      "loss": 2.6526,
      "step": 170
    },
    {
      "epoch": 5.423728813559322,
      "grad_norm": 0.027505118399858475,
      "learning_rate": 0.008199999999999999,
      "loss": 2.7324,
      "step": 180
    },
    {
      "epoch": 5.725047080979285,
      "grad_norm": 0.023669935762882233,
      "learning_rate": 0.008100000000000001,
      "loss": 2.7482,
      "step": 190
    },
    {
      "epoch": 6.0263653483992465,
      "grad_norm": 0.030947253108024597,
      "learning_rate": 0.008,
      "loss": 2.6727,
      "step": 200
    },
    {
      "epoch": 6.327683615819209,
      "grad_norm": 0.029249195009469986,
      "learning_rate": 0.0079,
      "loss": 2.6793,
      "step": 210
    },
    {
      "epoch": 6.629001883239171,
      "grad_norm": 0.03269331902265549,
      "learning_rate": 0.0078000000000000005,
      "loss": 2.7222,
      "step": 220
    },
    {
      "epoch": 6.930320150659134,
      "grad_norm": 0.030911127105355263,
      "learning_rate": 0.0077,
      "loss": 2.6732,
      "step": 230
    },
    {
      "epoch": 7.231638418079096,
      "grad_norm": 0.03097897581756115,
      "learning_rate": 0.0076,
      "loss": 2.6907,
      "step": 240
    },
    {
      "epoch": 7.532956685499059,
      "grad_norm": 0.0324222631752491,
      "learning_rate": 0.0075,
      "loss": 2.6541,
      "step": 250
    },
    {
      "epoch": 7.834274952919021,
      "grad_norm": 0.03252031281590462,
      "learning_rate": 0.0074,
      "loss": 2.7162,
      "step": 260
    },
    {
      "epoch": 8.135593220338983,
      "grad_norm": 0.04044438153505325,
      "learning_rate": 0.0073,
      "loss": 2.6865,
      "step": 270
    },
    {
      "epoch": 8.436911487758945,
      "grad_norm": 0.03537166118621826,
      "learning_rate": 0.0072,
      "loss": 2.7171,
      "step": 280
    },
    {
      "epoch": 8.738229755178908,
      "grad_norm": 0.034816764295101166,
      "learning_rate": 0.0070999999999999995,
      "loss": 2.695,
      "step": 290
    },
    {
      "epoch": 9.03954802259887,
      "grad_norm": 0.0346292108297348,
      "learning_rate": 0.006999999999999999,
      "loss": 2.6414,
      "step": 300
    },
    {
      "epoch": 9.340866290018832,
      "grad_norm": 0.046004340052604675,
      "learning_rate": 0.0069,
      "loss": 2.6881,
      "step": 310
    },
    {
      "epoch": 9.642184557438794,
      "grad_norm": 0.04795786738395691,
      "learning_rate": 0.0068000000000000005,
      "loss": 2.7072,
      "step": 320
    },
    {
      "epoch": 9.943502824858758,
      "grad_norm": 0.05860690027475357,
      "learning_rate": 0.0067,
      "loss": 2.6415,
      "step": 330
    },
    {
      "epoch": 10.24482109227872,
      "grad_norm": 0.049170173704624176,
      "learning_rate": 0.006600000000000001,
      "loss": 2.6163,
      "step": 340
    },
    {
      "epoch": 10.546139359698682,
      "grad_norm": 0.04039924964308739,
      "learning_rate": 0.006500000000000001,
      "loss": 2.6488,
      "step": 350
    },
    {
      "epoch": 10.847457627118644,
      "grad_norm": 0.042520683258771896,
      "learning_rate": 0.0064,
      "loss": 2.7223,
      "step": 360
    },
    {
      "epoch": 11.148775894538606,
      "grad_norm": 0.03674214333295822,
      "learning_rate": 0.0063,
      "loss": 2.7276,
      "step": 370
    },
    {
      "epoch": 11.45009416195857,
      "grad_norm": 0.03610653430223465,
      "learning_rate": 0.0062,
      "loss": 2.603,
      "step": 380
    },
    {
      "epoch": 11.751412429378531,
      "grad_norm": 0.0394720695912838,
      "learning_rate": 0.0061,
      "loss": 2.6998,
      "step": 390
    },
    {
      "epoch": 12.052730696798493,
      "grad_norm": 0.03712957352399826,
      "learning_rate": 0.006,
      "loss": 2.6505,
      "step": 400
    },
    {
      "epoch": 12.354048964218455,
      "grad_norm": 0.03861182555556297,
      "learning_rate": 0.0059,
      "loss": 2.6574,
      "step": 410
    },
    {
      "epoch": 12.655367231638419,
      "grad_norm": 0.03598365932703018,
      "learning_rate": 0.0058,
      "loss": 2.6554,
      "step": 420
    },
    {
      "epoch": 12.95668549905838,
      "grad_norm": 0.04243338480591774,
      "learning_rate": 0.005699999999999999,
      "loss": 2.6689,
      "step": 430
    },
    {
      "epoch": 13.258003766478343,
      "grad_norm": 0.04354375600814819,
      "learning_rate": 0.005600000000000001,
      "loss": 2.5795,
      "step": 440
    },
    {
      "epoch": 13.559322033898304,
      "grad_norm": 0.049008917063474655,
      "learning_rate": 0.0055000000000000005,
      "loss": 2.6327,
      "step": 450
    },
    {
      "epoch": 13.860640301318268,
      "grad_norm": 0.04114067554473877,
      "learning_rate": 0.0054,
      "loss": 2.7071,
      "step": 460
    },
    {
      "epoch": 14.16195856873823,
      "grad_norm": 0.04116629436612129,
      "learning_rate": 0.0053,
      "loss": 2.6687,
      "step": 470
    },
    {
      "epoch": 14.463276836158192,
      "grad_norm": 0.042137742042541504,
      "learning_rate": 0.005200000000000001,
      "loss": 2.6483,
      "step": 480
    },
    {
      "epoch": 14.764595103578154,
      "grad_norm": 0.03961750119924545,
      "learning_rate": 0.0051,
      "loss": 2.6318,
      "step": 490
    },
    {
      "epoch": 15.065913370998116,
      "grad_norm": 0.04454823583364487,
      "learning_rate": 0.005,
      "loss": 2.6189,
      "step": 500
    },
    {
      "epoch": 15.36723163841808,
      "grad_norm": 0.04488145187497139,
      "learning_rate": 0.0049,
      "loss": 2.6418,
      "step": 510
    },
    {
      "epoch": 15.668549905838042,
      "grad_norm": 0.038276150822639465,
      "learning_rate": 0.0048,
      "loss": 2.5911,
      "step": 520
    },
    {
      "epoch": 15.969868173258003,
      "grad_norm": 0.04120752587914467,
      "learning_rate": 0.0047,
      "loss": 2.7269,
      "step": 530
    },
    {
      "epoch": 16.271186440677965,
      "grad_norm": 0.04501068592071533,
      "learning_rate": 0.0046,
      "loss": 2.6478,
      "step": 540
    },
    {
      "epoch": 16.572504708097927,
      "grad_norm": 0.05031473562121391,
      "learning_rate": 0.0045000000000000005,
      "loss": 2.6534,
      "step": 550
    },
    {
      "epoch": 16.87382297551789,
      "grad_norm": 0.04089454934000969,
      "learning_rate": 0.0044,
      "loss": 2.6079,
      "step": 560
    },
    {
      "epoch": 17.175141242937855,
      "grad_norm": 0.040501464158296585,
      "learning_rate": 0.0043,
      "loss": 2.6138,
      "step": 570
    },
    {
      "epoch": 17.476459510357817,
      "grad_norm": 0.04049193859100342,
      "learning_rate": 0.0042,
      "loss": 2.6405,
      "step": 580
    },
    {
      "epoch": 17.77777777777778,
      "grad_norm": 0.04220786318182945,
      "learning_rate": 0.0040999999999999995,
      "loss": 2.6665,
      "step": 590
    },
    {
      "epoch": 18.07909604519774,
      "grad_norm": 0.045292966067790985,
      "learning_rate": 0.004,
      "loss": 2.6642,
      "step": 600
    },
    {
      "epoch": 18.380414312617702,
      "grad_norm": 0.04166285693645477,
      "learning_rate": 0.0039000000000000003,
      "loss": 2.6272,
      "step": 610
    },
    {
      "epoch": 18.681732580037664,
      "grad_norm": 0.040916282683610916,
      "learning_rate": 0.0038,
      "loss": 2.6069,
      "step": 620
    },
    {
      "epoch": 18.983050847457626,
      "grad_norm": 0.041318267583847046,
      "learning_rate": 0.0037,
      "loss": 2.6505,
      "step": 630
    },
    {
      "epoch": 19.284369114877588,
      "grad_norm": 0.04311233386397362,
      "learning_rate": 0.0036,
      "loss": 2.6345,
      "step": 640
    },
    {
      "epoch": 19.58568738229755,
      "grad_norm": 0.04370424896478653,
      "learning_rate": 0.0034999999999999996,
      "loss": 2.6724,
      "step": 650
    },
    {
      "epoch": 19.887005649717516,
      "grad_norm": 0.04762624204158783,
      "learning_rate": 0.0034000000000000002,
      "loss": 2.5742,
      "step": 660
    },
    {
      "epoch": 20.188323917137478,
      "grad_norm": 0.0427309013903141,
      "learning_rate": 0.0033000000000000004,
      "loss": 2.6244,
      "step": 670
    },
    {
      "epoch": 20.48964218455744,
      "grad_norm": 0.04483160376548767,
      "learning_rate": 0.0032,
      "loss": 2.6426,
      "step": 680
    },
    {
      "epoch": 20.7909604519774,
      "grad_norm": 0.04406247287988663,
      "learning_rate": 0.0031,
      "loss": 2.6359,
      "step": 690
    },
    {
      "epoch": 21.092278719397363,
      "grad_norm": 0.05134636163711548,
      "learning_rate": 0.003,
      "loss": 2.593,
      "step": 700
    },
    {
      "epoch": 21.393596986817325,
      "grad_norm": 0.0499412976205349,
      "learning_rate": 0.0029,
      "loss": 2.6529,
      "step": 710
    },
    {
      "epoch": 21.694915254237287,
      "grad_norm": 0.04201861098408699,
      "learning_rate": 0.0028000000000000004,
      "loss": 2.5882,
      "step": 720
    },
    {
      "epoch": 21.99623352165725,
      "grad_norm": 0.045000314712524414,
      "learning_rate": 0.0027,
      "loss": 2.6312,
      "step": 730
    },
    {
      "epoch": 22.29755178907721,
      "grad_norm": 0.051569245755672455,
      "learning_rate": 0.0026000000000000003,
      "loss": 2.6038,
      "step": 740
    },
    {
      "epoch": 22.598870056497177,
      "grad_norm": 0.04793541133403778,
      "learning_rate": 0.0025,
      "loss": 2.6085,
      "step": 750
    },
    {
      "epoch": 22.90018832391714,
      "grad_norm": 0.04966659098863602,
      "learning_rate": 0.0024,
      "loss": 2.6284,
      "step": 760
    },
    {
      "epoch": 23.2015065913371,
      "grad_norm": 0.042495761066675186,
      "learning_rate": 0.0023,
      "loss": 2.6351,
      "step": 770
    },
    {
      "epoch": 23.502824858757062,
      "grad_norm": 0.041642557829618454,
      "learning_rate": 0.0022,
      "loss": 2.5782,
      "step": 780
    },
    {
      "epoch": 23.804143126177024,
      "grad_norm": 0.04579734429717064,
      "learning_rate": 0.0021,
      "loss": 2.584,
      "step": 790
    },
    {
      "epoch": 24.105461393596986,
      "grad_norm": 0.04191246256232262,
      "learning_rate": 0.002,
      "loss": 2.666,
      "step": 800
    },
    {
      "epoch": 24.406779661016948,
      "grad_norm": 0.045601364225149155,
      "learning_rate": 0.0019,
      "loss": 2.5667,
      "step": 810
    },
    {
      "epoch": 24.70809792843691,
      "grad_norm": 0.048954639583826065,
      "learning_rate": 0.0018,
      "loss": 2.6866,
      "step": 820
    },
    {
      "epoch": 25.009416195856875,
      "grad_norm": 0.055057819932699203,
      "learning_rate": 0.0017000000000000001,
      "loss": 2.6051,
      "step": 830
    },
    {
      "epoch": 25.310734463276837,
      "grad_norm": 0.04590602219104767,
      "learning_rate": 0.0016,
      "loss": 2.6091,
      "step": 840
    },
    {
      "epoch": 25.6120527306968,
      "grad_norm": 0.043111998587846756,
      "learning_rate": 0.0015,
      "loss": 2.5334,
      "step": 850
    },
    {
      "epoch": 25.91337099811676,
      "grad_norm": 0.04524906724691391,
      "learning_rate": 0.0014000000000000002,
      "loss": 2.6581,
      "step": 860
    },
    {
      "epoch": 26.214689265536723,
      "grad_norm": 0.04507637768983841,
      "learning_rate": 0.0013000000000000002,
      "loss": 2.6312,
      "step": 870
    },
    {
      "epoch": 26.516007532956685,
      "grad_norm": 0.047058023512363434,
      "learning_rate": 0.0012,
      "loss": 2.6537,
      "step": 880
    },
    {
      "epoch": 26.817325800376647,
      "grad_norm": 0.04237693175673485,
      "learning_rate": 0.0011,
      "loss": 2.5716,
      "step": 890
    },
    {
      "epoch": 27.11864406779661,
      "grad_norm": 0.041641075164079666,
      "learning_rate": 0.001,
      "loss": 2.587,
      "step": 900
    },
    {
      "epoch": 27.41996233521657,
      "grad_norm": 0.043018363416194916,
      "learning_rate": 0.0009,
      "loss": 2.65,
      "step": 910
    },
    {
      "epoch": 27.721280602636536,
      "grad_norm": 0.04350588843226433,
      "learning_rate": 0.0008,
      "loss": 2.5523,
      "step": 920
    },
    {
      "epoch": 28.0225988700565,
      "grad_norm": 0.05018189176917076,
      "learning_rate": 0.0007000000000000001,
      "loss": 2.62,
      "step": 930
    },
    {
      "epoch": 28.32391713747646,
      "grad_norm": 0.04492446780204773,
      "learning_rate": 0.0006,
      "loss": 2.6057,
      "step": 940
    },
    {
      "epoch": 28.625235404896422,
      "grad_norm": 0.04419976845383644,
      "learning_rate": 0.0005,
      "loss": 2.597,
      "step": 950
    },
    {
      "epoch": 28.926553672316384,
      "grad_norm": 0.043191395699977875,
      "learning_rate": 0.0004,
      "loss": 2.5988,
      "step": 960
    },
    {
      "epoch": 29.227871939736346,
      "grad_norm": 0.04183311387896538,
      "learning_rate": 0.0003,
      "loss": 2.6207,
      "step": 970
    },
    {
      "epoch": 29.529190207156308,
      "grad_norm": 0.04085983708500862,
      "learning_rate": 0.0002,
      "loss": 2.6036,
      "step": 980
    },
    {
      "epoch": 29.83050847457627,
      "grad_norm": 0.041584163904190063,
      "learning_rate": 0.0001,
      "loss": 2.5908,
      "step": 990
    },
    {
      "epoch": 30.131826741996232,
      "grad_norm": 0.04027863219380379,
      "learning_rate": 0.0,
      "loss": 2.6037,
      "step": 1000
    },
    {
      "epoch": 30.131826741996232,
      "step": 1000,
      "total_flos": 5.882935998047846e+17,
      "train_loss": 2.6854229736328126,
      "train_runtime": 21552.388,
      "train_samples_per_second": 1.485,
      "train_steps_per_second": 0.046
    }
  ],
  "logging_steps": 10,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 31,
  "save_steps": 100,
  "total_flos": 5.882935998047846e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
