{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 15.065913370998116,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.3013182674199623,
      "grad_norm": 0.041579462587833405,
      "learning_rate": 0.0099,
      "loss": 4.4638,
      "step": 10
    },
    {
      "epoch": 0.6026365348399246,
      "grad_norm": 0.042861633002758026,
      "learning_rate": 0.0098,
      "loss": 3.3899,
      "step": 20
    },
    {
      "epoch": 0.903954802259887,
      "grad_norm": 0.019731702283024788,
      "learning_rate": 0.0097,
      "loss": 2.9816,
      "step": 30
    },
    {
      "epoch": 1.2052730696798493,
      "grad_norm": 0.00803113542497158,
      "learning_rate": 0.0096,
      "loss": 2.8857,
      "step": 40
    },
    {
      "epoch": 1.5065913370998116,
      "grad_norm": 0.006006977520883083,
      "learning_rate": 0.0095,
      "loss": 2.8,
      "step": 50
    },
    {
      "epoch": 1.807909604519774,
      "grad_norm": 0.006892246659845114,
      "learning_rate": 0.0094,
      "loss": 2.828,
      "step": 60
    },
    {
      "epoch": 2.109227871939736,
      "grad_norm": 0.0068437703885138035,
      "learning_rate": 0.009300000000000001,
      "loss": 2.7785,
      "step": 70
    },
    {
      "epoch": 2.4105461393596985,
      "grad_norm": 0.006782567128539085,
      "learning_rate": 0.0092,
      "loss": 2.7223,
      "step": 80
    },
    {
      "epoch": 2.711864406779661,
      "grad_norm": 0.011153936386108398,
      "learning_rate": 0.0091,
      "loss": 2.7488,
      "step": 90
    },
    {
      "epoch": 3.0131826741996233,
      "grad_norm": 0.016328509896993637,
      "learning_rate": 0.009000000000000001,
      "loss": 2.766,
      "step": 100
    },
    {
      "epoch": 3.3145009416195856,
      "grad_norm": 0.02455553039908409,
      "learning_rate": 0.0089,
      "loss": 2.789,
      "step": 110
    },
    {
      "epoch": 3.615819209039548,
      "grad_norm": 0.015264970250427723,
      "learning_rate": 0.0088,
      "loss": 2.6697,
      "step": 120
    },
    {
      "epoch": 3.9171374764595104,
      "grad_norm": 0.021795786917209625,
      "learning_rate": 0.0087,
      "loss": 2.7002,
      "step": 130
    },
    {
      "epoch": 4.218455743879472,
      "grad_norm": 0.022938553243875504,
      "learning_rate": 0.0086,
      "loss": 2.7149,
      "step": 140
    },
    {
      "epoch": 4.519774011299435,
      "grad_norm": 0.032675404101610184,
      "learning_rate": 0.0085,
      "loss": 2.7192,
      "step": 150
    },
    {
      "epoch": 4.821092278719397,
      "grad_norm": 0.03407915309071541,
      "learning_rate": 0.0084,
      "loss": 2.6928,
      "step": 160
    },
    {
      "epoch": 5.12241054613936,
      "grad_norm": 0.03350664675235748,
      "learning_rate": 0.0083,
      "loss": 2.6526,
      "step": 170
    },
    {
      "epoch": 5.423728813559322,
      "grad_norm": 0.027505118399858475,
      "learning_rate": 0.008199999999999999,
      "loss": 2.7324,
      "step": 180
    },
    {
      "epoch": 5.725047080979285,
      "grad_norm": 0.023669935762882233,
      "learning_rate": 0.008100000000000001,
      "loss": 2.7482,
      "step": 190
    },
    {
      "epoch": 6.0263653483992465,
      "grad_norm": 0.030947253108024597,
      "learning_rate": 0.008,
      "loss": 2.6727,
      "step": 200
    },
    {
      "epoch": 6.327683615819209,
      "grad_norm": 0.029249195009469986,
      "learning_rate": 0.0079,
      "loss": 2.6793,
      "step": 210
    },
    {
      "epoch": 6.629001883239171,
      "grad_norm": 0.03269331902265549,
      "learning_rate": 0.0078000000000000005,
      "loss": 2.7222,
      "step": 220
    },
    {
      "epoch": 6.930320150659134,
      "grad_norm": 0.030911127105355263,
      "learning_rate": 0.0077,
      "loss": 2.6732,
      "step": 230
    },
    {
      "epoch": 7.231638418079096,
      "grad_norm": 0.03097897581756115,
      "learning_rate": 0.0076,
      "loss": 2.6907,
      "step": 240
    },
    {
      "epoch": 7.532956685499059,
      "grad_norm": 0.0324222631752491,
      "learning_rate": 0.0075,
      "loss": 2.6541,
      "step": 250
    },
    {
      "epoch": 7.834274952919021,
      "grad_norm": 0.03252031281590462,
      "learning_rate": 0.0074,
      "loss": 2.7162,
      "step": 260
    },
    {
      "epoch": 8.135593220338983,
      "grad_norm": 0.04044438153505325,
      "learning_rate": 0.0073,
      "loss": 2.6865,
      "step": 270
    },
    {
      "epoch": 8.436911487758945,
      "grad_norm": 0.03537166118621826,
      "learning_rate": 0.0072,
      "loss": 2.7171,
      "step": 280
    },
    {
      "epoch": 8.738229755178908,
      "grad_norm": 0.034816764295101166,
      "learning_rate": 0.0070999999999999995,
      "loss": 2.695,
      "step": 290
    },
    {
      "epoch": 9.03954802259887,
      "grad_norm": 0.0346292108297348,
      "learning_rate": 0.006999999999999999,
      "loss": 2.6414,
      "step": 300
    },
    {
      "epoch": 9.340866290018832,
      "grad_norm": 0.046004340052604675,
      "learning_rate": 0.0069,
      "loss": 2.6881,
      "step": 310
    },
    {
      "epoch": 9.642184557438794,
      "grad_norm": 0.04795786738395691,
      "learning_rate": 0.0068000000000000005,
      "loss": 2.7072,
      "step": 320
    },
    {
      "epoch": 9.943502824858758,
      "grad_norm": 0.05860690027475357,
      "learning_rate": 0.0067,
      "loss": 2.6415,
      "step": 330
    },
    {
      "epoch": 10.24482109227872,
      "grad_norm": 0.049170173704624176,
      "learning_rate": 0.006600000000000001,
      "loss": 2.6163,
      "step": 340
    },
    {
      "epoch": 10.546139359698682,
      "grad_norm": 0.04039924964308739,
      "learning_rate": 0.006500000000000001,
      "loss": 2.6488,
      "step": 350
    },
    {
      "epoch": 10.847457627118644,
      "grad_norm": 0.042520683258771896,
      "learning_rate": 0.0064,
      "loss": 2.7223,
      "step": 360
    },
    {
      "epoch": 11.148775894538606,
      "grad_norm": 0.03674214333295822,
      "learning_rate": 0.0063,
      "loss": 2.7276,
      "step": 370
    },
    {
      "epoch": 11.45009416195857,
      "grad_norm": 0.03610653430223465,
      "learning_rate": 0.0062,
      "loss": 2.603,
      "step": 380
    },
    {
      "epoch": 11.751412429378531,
      "grad_norm": 0.0394720695912838,
      "learning_rate": 0.0061,
      "loss": 2.6998,
      "step": 390
    },
    {
      "epoch": 12.052730696798493,
      "grad_norm": 0.03712957352399826,
      "learning_rate": 0.006,
      "loss": 2.6505,
      "step": 400
    },
    {
      "epoch": 12.354048964218455,
      "grad_norm": 0.03861182555556297,
      "learning_rate": 0.0059,
      "loss": 2.6574,
      "step": 410
    },
    {
      "epoch": 12.655367231638419,
      "grad_norm": 0.03598365932703018,
      "learning_rate": 0.0058,
      "loss": 2.6554,
      "step": 420
    },
    {
      "epoch": 12.95668549905838,
      "grad_norm": 0.04243338480591774,
      "learning_rate": 0.005699999999999999,
      "loss": 2.6689,
      "step": 430
    },
    {
      "epoch": 13.258003766478343,
      "grad_norm": 0.04354375600814819,
      "learning_rate": 0.005600000000000001,
      "loss": 2.5795,
      "step": 440
    },
    {
      "epoch": 13.559322033898304,
      "grad_norm": 0.049008917063474655,
      "learning_rate": 0.0055000000000000005,
      "loss": 2.6327,
      "step": 450
    },
    {
      "epoch": 13.860640301318268,
      "grad_norm": 0.04114067554473877,
      "learning_rate": 0.0054,
      "loss": 2.7071,
      "step": 460
    },
    {
      "epoch": 14.16195856873823,
      "grad_norm": 0.04116629436612129,
      "learning_rate": 0.0053,
      "loss": 2.6687,
      "step": 470
    },
    {
      "epoch": 14.463276836158192,
      "grad_norm": 0.042137742042541504,
      "learning_rate": 0.005200000000000001,
      "loss": 2.6483,
      "step": 480
    },
    {
      "epoch": 14.764595103578154,
      "grad_norm": 0.03961750119924545,
      "learning_rate": 0.0051,
      "loss": 2.6318,
      "step": 490
    },
    {
      "epoch": 15.065913370998116,
      "grad_norm": 0.04454823583364487,
      "learning_rate": 0.005,
      "loss": 2.6189,
      "step": 500
    }
  ],
  "logging_steps": 10,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 31,
  "save_steps": 100,
  "total_flos": 2.941467999023923e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
